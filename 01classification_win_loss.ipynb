{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in full data set\n",
    "full_ = pd.read_csv('all_features.csv.gz', compression = 'gzip')\n",
    "#drop merge keys and non-numeric columns\n",
    "full_ = full_.drop(columns = ['date', 'team1', 'team2', 'home_starter', 'road_starter',\n",
    "                             'is_doubleheader', 'is_tripleheader', 'score1', 'score2'])\n",
    "X, y = full_.drop(columns = ['home_loss']), full_['home_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline logistic regression accuracy: 0.5736283947718557\n",
      "\n",
      "Baseline SGD Classifier accuracy 0.4848947327228614\n"
     ]
    }
   ],
   "source": [
    "#split into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "#train and test baseline logistic regression model\n",
    "lr = LogisticRegression(solver = 'lbfgs', max_iter = 1000)\n",
    "lr.fit(x_train, y_train)\n",
    "lr_preds = lr.predict(x_test)\n",
    "lr_accuracy = accuracy_score(y_test, lr_preds)\n",
    "#train and test baseline SGD Classifier\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(x_train, y_train)\n",
    "sgd_preds = sgd.predict(x_test)\n",
    "sgd_accuracy = accuracy_score(y_test, sgd_preds)\n",
    "print('Baseline logistic regression accuracy: {}'.format(lr_accuracy))\n",
    "print()\n",
    "print('Baseline SGD Classifier accuracy {}'.format(sgd_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREE BASED MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recursive_selection import FeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "7 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n"
     ]
    }
   ],
   "source": [
    "#generate dictionary of algorithms to select features based on feature importance and specify generic base\n",
    "#parameters to be passed to FeatureSelector object\n",
    "tree_based = {'RandomForestClassifier': (RandomForestClassifier(), {'n_estimators' : 100,\n",
    "                                                                   'max_depth' : 7}),\n",
    "             'LGBMClassifier' : (lgb.LGBMClassifier(), {'num_leaves' : 70,\n",
    "                                                       'max_depth' : 6}),\n",
    "             'XGBClassifier' : (xgb.XGBClassifier(), {'max_depth' : 5,\n",
    "                                                     'n_estimators' : 100})}\n",
    "#function to initialize results dictionary for each algorithm to be called to compare results\n",
    "def initialize_results_dict():\n",
    "    return( {'Algorithm' : None,\n",
    "            'best_eval_full' : None,\n",
    "            'best_subset_full' : None,\n",
    "            'best_eval_drop_corr' : None,\n",
    "            'best_subset_drop_corr' : None})\n",
    "#empty list to store result dictionaries- can be easily made into dataframe using pd.concat\n",
    "results_list = []\n",
    "\n",
    "#iterate through algorithms, use recursive_selection method of FeatureSelector object, first using full feature set,\n",
    "#then eliminating correlated features (with given tolerance)\n",
    "for algo in tree_based:\n",
    "    result = initialize_results_dict()\n",
    "    result['Algorithm'] = algo\n",
    "    selector = FeatureSelector(X, y, algorithm = tree_based[algo][0], params = tree_based[algo][1], drop_size = 10)\n",
    "    selector.recursive_selection()\n",
    "    result['best_eval_full'] = selector.best_eval\n",
    "    result['best_subset_full'] = selector.best_subset\n",
    "    selector = FeatureSelector(X, y, algorithm = tree_based[algo][0], drop_corr = True,\n",
    "                               params = tree_based[algo][1], drop_size = 10, correlation_tolerance = 0.6)\n",
    "    selector.recursive_selection()\n",
    "    result['best_eval_drop_corr'] = selector.best_eval\n",
    "    result['best_subset_drop_corr'] = selector.best_subset\n",
    "    results_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe to evaluate results\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>best_eval_full</th>\n",
       "      <th>best_subset_full</th>\n",
       "      <th>best_eval_drop_corr</th>\n",
       "      <th>best_subset_drop_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.573724</td>\n",
       "      <td>Index(['elo1_pre', 'elo_prob1', 'elo_prob2', '...</td>\n",
       "      <td>0.573653</td>\n",
       "      <td>Index(['road_OBPS', 'road_career_ERA', 'elo1_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.571153</td>\n",
       "      <td>Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...</td>\n",
       "      <td>0.571029</td>\n",
       "      <td>Index(['home_OBPS', 'elo1_pre', 'elo2_pre', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.573360</td>\n",
       "      <td>Index(['home_OBPS', 'home_AVG_RUNS', 'home_tot...</td>\n",
       "      <td>0.572139</td>\n",
       "      <td>Index(['home_OBPS', 'home_BULLPEN_ERA', 'home_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Algorithm  best_eval_full  \\\n",
       "0  RandomForestClassifier        0.573724   \n",
       "1          LGBMClassifier        0.571153   \n",
       "2           XGBClassifier        0.573360   \n",
       "\n",
       "                                    best_subset_full  best_eval_drop_corr  \\\n",
       "0  Index(['elo1_pre', 'elo_prob1', 'elo_prob2', '...             0.573653   \n",
       "1  Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...             0.571029   \n",
       "2  Index(['home_OBPS', 'home_AVG_RUNS', 'home_tot...             0.572139   \n",
       "\n",
       "                               best_subset_drop_corr  \n",
       "0  Index(['road_OBPS', 'road_career_ERA', 'elo1_p...  \n",
       "1  Index(['home_OBPS', 'elo1_pre', 'elo2_pre', 'p...  \n",
       "2  Index(['home_OBPS', 'home_BULLPEN_ERA', 'home_...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review dataframe \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "7 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n"
     ]
    }
   ],
   "source": [
    "#new result list\n",
    "results_list = []\n",
    "\n",
    "#iterate through algorithms, use recursive_selection method of FeatureSelector object, first using full feature set,\n",
    "#then eliminating correlated features (with given tolerance), this time with scaled features\n",
    "for algo in tree_based:\n",
    "    result = initialize_results_dict()\n",
    "    result['Algorithm'] = algo\n",
    "    selector = FeatureSelector(X, y, algorithm = tree_based[algo][0], scale = 'standard', \n",
    "                               params = tree_based[algo][1], drop_size = 10)\n",
    "    selector.recursive_selection()\n",
    "    result['best_eval_full'] = selector.best_eval\n",
    "    result['best_subset_full'] = selector.best_subset\n",
    "    selector = FeatureSelector(X, y, algorithm = tree_based[algo][0], scale = 'standard', drop_corr = True,\n",
    "                               params = tree_based[algo][1], drop_size = 10, correlation_tolerance = 0.6)\n",
    "    selector.recursive_selection()\n",
    "    result['best_eval_drop_corr'] = selector.best_eval\n",
    "    result['best_subset_drop_corr'] = selector.best_subset\n",
    "    results_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>best_eval_full</th>\n",
       "      <th>best_subset_full</th>\n",
       "      <th>best_eval_drop_corr</th>\n",
       "      <th>best_subset_drop_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.573958</td>\n",
       "      <td>Index(['elo1_pre', 'elo_prob1', 'elo_prob2', '...</td>\n",
       "      <td>0.573653</td>\n",
       "      <td>Index(['road_OBPS', 'road_career_ERA', 'elo1_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.571147</td>\n",
       "      <td>Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...</td>\n",
       "      <td>0.571065</td>\n",
       "      <td>Index(['home_OBPS', 'elo1_pre', 'elo2_pre', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.573201</td>\n",
       "      <td>Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...</td>\n",
       "      <td>0.572978</td>\n",
       "      <td>Index(['home_OBPS', 'home_BULLPEN_ERA', 'home_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Algorithm  best_eval_full  \\\n",
       "0  RandomForestClassifier        0.573958   \n",
       "1          LGBMClassifier        0.571147   \n",
       "2           XGBClassifier        0.573201   \n",
       "\n",
       "                                    best_subset_full  best_eval_drop_corr  \\\n",
       "0  Index(['elo1_pre', 'elo_prob1', 'elo_prob2', '...             0.573653   \n",
       "1  Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...             0.571065   \n",
       "2  Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...             0.572978   \n",
       "\n",
       "                               best_subset_drop_corr  \n",
       "0  Index(['road_OBPS', 'road_career_ERA', 'elo1_p...  \n",
       "1  Index(['home_OBPS', 'elo1_pre', 'elo2_pre', 'p...  \n",
       "2  Index(['home_OBPS', 'home_BULLPEN_ERA', 'home_...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate DataFrame of results for scaled data\n",
    "scaled_results_df = pd.DataFrame(results_list)\n",
    "scaled_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['elo1_pre', 'elo_prob1', 'elo_prob2', 'rating1_pre', 'rating2_pre',\n",
       "       'pitcher1_rgs', 'pitcher2_rgs', 'rating_prob1', 'rating_prob2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return highest performing subset\n",
    "best_ = scaled_results_df.iloc[scaled_results_df.best_eval_full.idxmax()]['best_subset_full']\n",
    "#view highest performing subset\n",
    "best_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter tuning with hyperopt, generate parameter space to search over\n",
    "hyperopt_space = {'criterion' : hp.choice('criterion', ['gini', 'entropy']),\n",
    "                  'n_estimators' : scope.int(hp.quniform('n_estimators', 10, 200, 10)),\n",
    "                  'max_depth' : scope.int(hp.quniform('max_depth', 3, 10, 1)),\n",
    "                  'max_features' : hp.choice('max_features', ['auto', None]),\n",
    "                  'bootstrap' : hp.choice('bootstrap', [True, False]),\n",
    "                  'n_jobs' : -1\n",
    "                 }\n",
    "#optimizer function to call fmin function\n",
    "def rfc_optimizer(param_space, x_train, y_train, num_eval):\n",
    "    #objective function to minimize \n",
    "    def objective(params):\n",
    "        rfc = RandomForestClassifier(**params)\n",
    "        score = cross_val_score(rfc, x_train, y_train, cv = 10).mean()\n",
    "        return({'loss' : -score, 'status' : STATUS_OK})\n",
    "    #initialize trials objects to record performance\n",
    "    trials = Trials()\n",
    "    \n",
    "    best_params = fmin(objective, param_space, algo = tpe.suggest, max_evals = num_eval, trials = trials,\n",
    "                      rstate = np.random.RandomState(32))\n",
    "    \n",
    "    return(best_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:00:13<00:00, 72.28s/it, best loss: -0.5741168825141509] \n"
     ]
    }
   ],
   "source": [
    "#use best subset generated by feature selection process\n",
    "X_best = X[best_]\n",
    "\n",
    "#call search function\n",
    "best_params = rfc_optimizer(hyperopt_space, X_best, y, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': 0,\n",
       " 'criterion': 0,\n",
       " 'max_depth': 6.0,\n",
       " 'max_features': 0,\n",
       " 'n_estimators': 30.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check best parameters\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of optimized RandomForestClassifier 0.5731392345621038\n"
     ]
    }
   ],
   "source": [
    "#fit final feature set and final parameter set and test performance\n",
    "rfc_optimized = RandomForestClassifier(n_estimators = 30, criterion = 'gini', max_depth = 6, max_features = 'auto',\n",
    "                                      bootstrap = True, n_jobs = -1)\n",
    "\n",
    "#split full dataset into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "#fit on training data\n",
    "rfc_optimized.fit(x_train, y_train)\n",
    "#predict on test data\n",
    "y_pred = rfc_optimized.predict(x_test)\n",
    "#score on validation set\n",
    "accuracy_rfc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of optimized RandomForestClassifier {}'.format(accuracy_rfc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elo_prob1     elo_prob2       1.000000\n",
       "rating_prob1  rating_prob2    1.000000\n",
       "elo1_pre      rating1_pre     0.995811\n",
       "elo_prob1     rating_prob2    0.970041\n",
       "              rating_prob1    0.970041\n",
       "elo_prob2     rating_prob2    0.970041\n",
       "              rating_prob1    0.970041\n",
       "elo1_pre      elo_prob2       0.742245\n",
       "              elo_prob1       0.742245\n",
       "elo_prob1     rating1_pre     0.739824\n",
       "elo_prob2     rating1_pre     0.739824\n",
       "elo_prob1     rating2_pre     0.736012\n",
       "elo_prob2     rating2_pre     0.736012\n",
       "rating1_pre   rating_prob2    0.722678\n",
       "              rating_prob1    0.722678\n",
       "elo1_pre      rating_prob2    0.719878\n",
       "              rating_prob1    0.719878\n",
       "rating2_pre   rating_prob2    0.719219\n",
       "              rating_prob1    0.719219\n",
       "elo1_pre      pitcher1_rgs    0.354917\n",
       "rating2_pre   pitcher2_rgs    0.350723\n",
       "rating1_pre   pitcher1_rgs    0.349292\n",
       "pitcher2_rgs  rating_prob2    0.288479\n",
       "              rating_prob1    0.288479\n",
       "pitcher1_rgs  rating_prob2    0.285707\n",
       "              rating_prob1    0.285707\n",
       "elo_prob1     pitcher2_rgs    0.251062\n",
       "elo_prob2     pitcher2_rgs    0.251062\n",
       "elo_prob1     pitcher1_rgs    0.246587\n",
       "elo_prob2     pitcher1_rgs    0.246587\n",
       "rating1_pre   rating2_pre     0.098538\n",
       "elo1_pre      rating2_pre     0.097461\n",
       "pitcher1_rgs  pitcher2_rgs    0.020801\n",
       "rating1_pre   pitcher2_rgs    0.016615\n",
       "elo1_pre      pitcher2_rgs    0.016353\n",
       "rating2_pre   pitcher1_rgs    0.009313\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for colinearity in final feature frame\n",
    "best_corr = X_best.corr().abs()\n",
    "all_correlated = best_corr.where(np.triu(np.ones(best_corr.shape), k = 1)\\\n",
    "                              .astype(np.bool)).stack().sort_values(ascending = False)\n",
    "#view colinearity\n",
    "all_correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop highly correlated features\n",
    "X_best = X_best.drop(columns = ['elo_prob2', 'rating_prob2', 'rating1_pre', 'elo_prob1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of optimized RandomForestClassifier 0.5730609689285435\n"
     ]
    }
   ],
   "source": [
    "#fit final feature set and final parameter set and test performance\n",
    "rfc_optimized = RandomForestClassifier(n_estimators = 30, criterion = 'gini', max_depth = 6, max_features = 'auto',\n",
    "                                      bootstrap = True, n_jobs = -1)\n",
    "\n",
    "#split full dataset into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "#fit on training data\n",
    "rfc_optimized.fit(x_train, y_train)\n",
    "#predict on test data\n",
    "y_pred = rfc_optimized.predict(x_test)\n",
    "#score on validation set\n",
    "accuracy_rfc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of optimized RandomForestClassifier {}'.format(accuracy_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of optimized RandomForestClassifier with scaled features is 0.5735501291382954\n"
     ]
    }
   ],
   "source": [
    "#check if scaling features will have any effect on final accuracy \n",
    "ss = StandardScaler()\n",
    "#fit training data\n",
    "ss.fit(x_train)\n",
    "\n",
    "#scale training and test data\n",
    "x_train_s, x_test_s = ss.transform(x_train), ss.transform(x_test)\n",
    "#reinitialize optimized RFC\n",
    "rfc_optimized = RandomForestClassifier(n_estimators = 30, criterion = 'gini', max_depth = 6, max_features = 'auto',\n",
    "                                      bootstrap = True, n_jobs = -1)\n",
    "#fit on training data\n",
    "rfc_optimized.fit(x_train_s, y_train)\n",
    "#predict on test data\n",
    "y_pred = rfc_optimized.predict(x_test_s)\n",
    "#score on validation set\n",
    "accuracy_rfc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy of optimized RandomForestClassifier with scaled features is {}'.format(accuracy_rfc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Win/Loss Classification\n",
    "Using the final dataframe collected and aggregated from FiveThirtyEight, baseball-reference.com, Retrosheet.org, and the NOAA Weather Service, (see data collection/exploration in this repo and in mlb_spread_betting repo) the end goal is to accurately model the spread. The first step in this process will be predicting the game winner, and then using this prediction as a feature to predict the spread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in full data set\n",
    "full_ = pd.read_csv('all_features.csv.gz', compression = 'gzip')\n",
    "#drop merge keys and non-numeric columns\n",
    "full_ = full_.drop(columns = ['date', 'team1', 'team2', 'home_starter', 'road_starter',\n",
    "                             'is_doubleheader', 'is_tripleheader', 'score1', 'score2'])\n",
    "X, y = full_.drop(columns = ['home_loss']), full_['home_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline logistic regression accuracy: 0.5736283947718557\n",
      "\n",
      "Baseline SGD Classifier accuracy 0.4848947327228614\n"
     ]
    }
   ],
   "source": [
    "#split into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "#train and test baseline logistic regression model\n",
    "lr = LogisticRegression(solver = 'lbfgs', max_iter = 1000)\n",
    "lr.fit(x_train, y_train)\n",
    "lr_preds = lr.predict(x_test)\n",
    "lr_accuracy = accuracy_score(y_test, lr_preds)\n",
    "#train and test baseline SGD Classifier\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(x_train, y_train)\n",
    "sgd_preds = sgd.predict(x_test)\n",
    "sgd_accuracy = accuracy_score(y_test, sgd_preds)\n",
    "print('Baseline logistic regression accuracy: {}'.format(lr_accuracy))\n",
    "print()\n",
    "print('Baseline SGD Classifier accuracy {}'.format(sgd_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREE BASED MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the final feature set to use in parameter tuning and model building, I have developed a __FeatureSelector__ object (see __recursive_selection.py__ for a full explanation of this object) to perform recursive features selection. While FeatureSelector offers several different methods of feature selection, with tree-based models I will use the feature importances attribute and iterate through the full feature frame, removing the least important features at each iteration and evaluating the new feature set using 5 fold cross validation with a given metric (in this case, with two balanced classes, it will be accuracy). I will perform this in several different cases: with the full feature frame, reduced feature frame after removing highly correlated features, and the feature frame with features normalized to mean 0 and unit variance. NOTE: Unless otherwise specified with n_jobs parameter, FeatureSelector will run all available processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recursive_selection import FeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "7 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n"
     ]
    }
   ],
   "source": [
    "#generate dictionary of algorithms to select features based on feature importance and specify generic base\n",
    "#parameters to be passed to FeatureSelector object\n",
    "tree_based = {'RandomForestClassifier': (RandomForestClassifier(), {'n_estimators' : 100,\n",
    "                                                                   'max_depth' : 7}),\n",
    "             'LGBMClassifier' : (lgb.LGBMClassifier(), {'num_leaves' : 70,\n",
    "                                                       'max_depth' : 6}),\n",
    "             'XGBClassifier' : (xgb.XGBClassifier(), {'max_depth' : 5,\n",
    "                                                     'n_estimators' : 100})}\n",
    "#function to initialize results dictionary for each algorithm to be called to compare results\n",
    "def initialize_results_dict():\n",
    "    return( {'Algorithm' : None,\n",
    "            'best_eval_full' : None,\n",
    "            'best_subset_full' : None,\n",
    "            'best_eval_drop_corr' : None,\n",
    "            'best_subset_drop_corr' : None})\n",
    "#empty list to store result dictionaries- can be easily made into dataframe using pd.concat\n",
    "results_list = []\n",
    "\n",
    "#iterate through algorithms, use recursive_selection method of FeatureSelector object, first using full feature set,\n",
    "#then eliminating correlated features (with given tolerance)\n",
    "for algo in tree_based:\n",
    "    result = initialize_results_dict()\n",
    "    result['Algorithm'] = algo\n",
    "    selector = FeatureSelector(X, y, algorithm = tree_based[algo][0], params = tree_based[algo][1], drop_size = 10)\n",
    "    selector.recursive_selection()\n",
    "    result['best_eval_full'] = selector.best_eval\n",
    "    result['best_subset_full'] = selector.best_subset\n",
    "    selector = FeatureSelector(X, y, algorithm = tree_based[algo][0], drop_corr = True,\n",
    "                               params = tree_based[algo][1], drop_size = 10, correlation_tolerance = 0.6)\n",
    "    selector.recursive_selection()\n",
    "    result['best_eval_drop_corr'] = selector.best_eval\n",
    "    result['best_subset_drop_corr'] = selector.best_subset\n",
    "    results_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe to evaluate results\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>best_eval_full</th>\n",
       "      <th>best_subset_full</th>\n",
       "      <th>best_eval_drop_corr</th>\n",
       "      <th>best_subset_drop_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.573724</td>\n",
       "      <td>Index(['elo1_pre', 'elo_prob1', 'elo_prob2', '...</td>\n",
       "      <td>0.573653</td>\n",
       "      <td>Index(['road_OBPS', 'road_career_ERA', 'elo1_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.571153</td>\n",
       "      <td>Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...</td>\n",
       "      <td>0.571029</td>\n",
       "      <td>Index(['home_OBPS', 'elo1_pre', 'elo2_pre', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.573360</td>\n",
       "      <td>Index(['home_OBPS', 'home_AVG_RUNS', 'home_tot...</td>\n",
       "      <td>0.572139</td>\n",
       "      <td>Index(['home_OBPS', 'home_BULLPEN_ERA', 'home_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Algorithm  best_eval_full  \\\n",
       "0  RandomForestClassifier        0.573724   \n",
       "1          LGBMClassifier        0.571153   \n",
       "2           XGBClassifier        0.573360   \n",
       "\n",
       "                                    best_subset_full  best_eval_drop_corr  \\\n",
       "0  Index(['elo1_pre', 'elo_prob1', 'elo_prob2', '...             0.573653   \n",
       "1  Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...             0.571029   \n",
       "2  Index(['home_OBPS', 'home_AVG_RUNS', 'home_tot...             0.572139   \n",
       "\n",
       "                               best_subset_drop_corr  \n",
       "0  Index(['road_OBPS', 'road_career_ERA', 'elo1_p...  \n",
       "1  Index(['home_OBPS', 'elo1_pre', 'elo2_pre', 'p...  \n",
       "2  Index(['home_OBPS', 'home_BULLPEN_ERA', 'home_...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review dataframe \n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from this table, the highest performing algorithm was the RandomForestClassifier, and the best subset is available. I will run the same method, but this time with a scaled feature frame to test if this improves performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "7 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "5 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n",
      "10 features have been dropped, moving to next iteration\n"
     ]
    }
   ],
   "source": [
    "#new result list\n",
    "results_list = []\n",
    "\n",
    "#iterate through algorithms, use recursive_selection method of FeatureSelector object, first using full feature set,\n",
    "#then eliminating correlated features (with given tolerance), this time with scaled features\n",
    "for algo in tree_based:\n",
    "    result = initialize_results_dict()\n",
    "    result['Algorithm'] = algo\n",
    "    selector = FeatureSelector(X, y, algorithm = tree_based[algo][0], scale = 'standard', \n",
    "                               params = tree_based[algo][1], drop_size = 10)\n",
    "    selector.recursive_selection()\n",
    "    result['best_eval_full'] = selector.best_eval\n",
    "    result['best_subset_full'] = selector.best_subset\n",
    "    selector = FeatureSelector(X, y, algorithm = tree_based[algo][0], scale = 'standard', drop_corr = True,\n",
    "                               params = tree_based[algo][1], drop_size = 10, correlation_tolerance = 0.6)\n",
    "    selector.recursive_selection()\n",
    "    result['best_eval_drop_corr'] = selector.best_eval\n",
    "    result['best_subset_drop_corr'] = selector.best_subset\n",
    "    results_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>best_eval_full</th>\n",
       "      <th>best_subset_full</th>\n",
       "      <th>best_eval_drop_corr</th>\n",
       "      <th>best_subset_drop_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.573958</td>\n",
       "      <td>Index(['elo1_pre', 'elo_prob1', 'elo_prob2', '...</td>\n",
       "      <td>0.573653</td>\n",
       "      <td>Index(['road_OBPS', 'road_career_ERA', 'elo1_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.571147</td>\n",
       "      <td>Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...</td>\n",
       "      <td>0.571065</td>\n",
       "      <td>Index(['home_OBPS', 'elo1_pre', 'elo2_pre', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.573201</td>\n",
       "      <td>Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...</td>\n",
       "      <td>0.572978</td>\n",
       "      <td>Index(['home_OBPS', 'home_BULLPEN_ERA', 'home_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Algorithm  best_eval_full  \\\n",
       "0  RandomForestClassifier        0.573958   \n",
       "1          LGBMClassifier        0.571147   \n",
       "2           XGBClassifier        0.573201   \n",
       "\n",
       "                                    best_subset_full  best_eval_drop_corr  \\\n",
       "0  Index(['elo1_pre', 'elo_prob1', 'elo_prob2', '...             0.573653   \n",
       "1  Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...             0.571065   \n",
       "2  Index(['home_OBPS', 'home_AVG_RUNS', 'home_AVG...             0.572978   \n",
       "\n",
       "                               best_subset_drop_corr  \n",
       "0  Index(['road_OBPS', 'road_career_ERA', 'elo1_p...  \n",
       "1  Index(['home_OBPS', 'elo1_pre', 'elo2_pre', 'p...  \n",
       "2  Index(['home_OBPS', 'home_BULLPEN_ERA', 'home_...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate DataFrame of results for scaled data\n",
    "scaled_results_df = pd.DataFrame(results_list)\n",
    "scaled_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier is again the best performer, and there is no marked improvement from scaling features. Now, I will perform hyperparameter tuning using the RandomForestClassifier and the best subset generated by the FeatureSelector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['elo1_pre', 'elo_prob1', 'elo_prob2', 'rating1_pre', 'rating2_pre',\n",
       "       'pitcher1_rgs', 'pitcher2_rgs', 'rating_prob1', 'rating_prob2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return highest performing subset\n",
    "best_ = results_df.iloc[results_df.best_eval_full.idxmax()]['best_subset_full']\n",
    "#view highest performing subset\n",
    "best_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parameter tuning, I have elected to use the Hyperopt library implementing Bayesian parameter tuning, using the tree-structured Parzen estimator algorithm (https://optunity.readthedocs.io/en/latest/user/solvers/TPE.html#hyperopt). Given a parameter space to search over, the optimizer function below uses the fmin function from the Hyperopt library to minimize an objective function, which in this case is cross validation score of the RandomForestClassifier with a given set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter tuning with hyperopt, generate parameter space to search over\n",
    "hyperopt_space = {'criterion' : hp.choice('criterion', ['gini', 'entropy']),\n",
    "                  'n_estimators' : scope.int(hp.quniform('n_estimators', 10, 200, 10)),\n",
    "                  'max_depth' : scope.int(hp.quniform('max_depth', 3, 10, 1)),\n",
    "                  'max_features' : hp.choice('max_features', ['auto', None]),\n",
    "                  'bootstrap' : hp.choice('bootstrap', [True, False]),\n",
    "                  'n_jobs' : -1\n",
    "                 }\n",
    "#optimizer function to call fmin function\n",
    "def rfc_optimizer(param_space, x_train, y_train, num_eval):\n",
    "    #objective function to minimize \n",
    "    def objective(params):\n",
    "        rfc = RandomForestClassifier(**params)\n",
    "        score = cross_val_score(rfc, x_train, y_train, cv = 10).mean()\n",
    "        return({'loss' : -score, 'status' : STATUS_OK})\n",
    "    #initialize trials objects to record performance\n",
    "    trials = Trials()\n",
    "    \n",
    "    best_params = fmin(objective, param_space, algo = tpe.suggest, max_evals = num_eval, trials = trials,\n",
    "                      rstate = np.random.RandomState(32))\n",
    "    \n",
    "    return(best_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:00:13<00:00, 72.28s/it, best loss: -0.5741168825141509] \n"
     ]
    }
   ],
   "source": [
    "#use best subset generated by feature selection process\n",
    "X_best = X[best_]\n",
    "\n",
    "#call search function\n",
    "best_params = rfc_optimizer(hyperopt_space, X_best, y, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': 0,\n",
       " 'criterion': 0,\n",
       " 'max_depth': 6.0,\n",
       " 'max_features': 0,\n",
       " 'n_estimators': 30.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check best parameters\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the best parameters from the search, I will now fit on a training set and test on a holdout set for a final validation accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of optimized RandomForestClassifier 0.5731392345621038\n"
     ]
    }
   ],
   "source": [
    "#fit final feature set and final parameter set and test performance\n",
    "rfc_optimized = RandomForestClassifier(n_estimators = 30, criterion = 'gini', max_depth = 6, max_features = 'auto',\n",
    "                                      bootstrap = True, n_jobs = -1)\n",
    "\n",
    "#split full dataset into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "#fit on training data\n",
    "rfc_optimized.fit(x_train, y_train)\n",
    "#predict on test data\n",
    "y_pred = rfc_optimized.predict(x_test)\n",
    "#score on validation set\n",
    "accuracy_rfc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of optimized RandomForestClassifier {}'.format(accuracy_rfc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model did not improve upon the baseline LogisticRegression model. I will try removing features with high colinearity and scaling the features to determine if this will improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elo_prob1     elo_prob2       1.000000\n",
       "rating_prob1  rating_prob2    1.000000\n",
       "elo1_pre      rating1_pre     0.995811\n",
       "elo_prob1     rating_prob2    0.970041\n",
       "              rating_prob1    0.970041\n",
       "elo_prob2     rating_prob2    0.970041\n",
       "              rating_prob1    0.970041\n",
       "elo1_pre      elo_prob2       0.742245\n",
       "              elo_prob1       0.742245\n",
       "elo_prob1     rating1_pre     0.739824\n",
       "elo_prob2     rating1_pre     0.739824\n",
       "elo_prob1     rating2_pre     0.736012\n",
       "elo_prob2     rating2_pre     0.736012\n",
       "rating1_pre   rating_prob2    0.722678\n",
       "              rating_prob1    0.722678\n",
       "elo1_pre      rating_prob2    0.719878\n",
       "              rating_prob1    0.719878\n",
       "rating2_pre   rating_prob2    0.719219\n",
       "              rating_prob1    0.719219\n",
       "elo1_pre      pitcher1_rgs    0.354917\n",
       "rating2_pre   pitcher2_rgs    0.350723\n",
       "rating1_pre   pitcher1_rgs    0.349292\n",
       "pitcher2_rgs  rating_prob2    0.288479\n",
       "              rating_prob1    0.288479\n",
       "pitcher1_rgs  rating_prob2    0.285707\n",
       "              rating_prob1    0.285707\n",
       "elo_prob1     pitcher2_rgs    0.251062\n",
       "elo_prob2     pitcher2_rgs    0.251062\n",
       "elo_prob1     pitcher1_rgs    0.246587\n",
       "elo_prob2     pitcher1_rgs    0.246587\n",
       "rating1_pre   rating2_pre     0.098538\n",
       "elo1_pre      rating2_pre     0.097461\n",
       "pitcher1_rgs  pitcher2_rgs    0.020801\n",
       "rating1_pre   pitcher2_rgs    0.016615\n",
       "elo1_pre      pitcher2_rgs    0.016353\n",
       "rating2_pre   pitcher1_rgs    0.009313\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for colinearity in final feature frame\n",
    "best_corr = X_best.corr().abs()\n",
    "all_correlated = best_corr.where(np.triu(np.ones(best_corr.shape), k = 1)\\\n",
    "                              .astype(np.bool)).stack().sort_values(ascending = False)\n",
    "#view colinearity\n",
    "all_correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop highly correlated features\n",
    "X_best = X_best.drop(columns = ['elo_prob2', 'rating_prob2', 'rating1_pre', 'elo_prob1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of optimized RandomForestClassifier 0.5730609689285435\n"
     ]
    }
   ],
   "source": [
    "#fit final feature set and final parameter set and test performance\n",
    "rfc_optimized = RandomForestClassifier(n_estimators = 30, criterion = 'gini', max_depth = 6, max_features = 'auto',\n",
    "                                      bootstrap = True, n_jobs = -1)\n",
    "\n",
    "#split full dataset into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "#fit on training data\n",
    "rfc_optimized.fit(x_train, y_train)\n",
    "#predict on test data\n",
    "y_pred = rfc_optimized.predict(x_test)\n",
    "#score on validation set\n",
    "accuracy_rfc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of optimized RandomForestClassifier {}'.format(accuracy_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of optimized RandomForestClassifier with scaled features is 0.5735501291382954\n"
     ]
    }
   ],
   "source": [
    "#check if scaling features will have any effect on final accuracy \n",
    "ss = StandardScaler()\n",
    "#fit training data\n",
    "ss.fit(x_train)\n",
    "\n",
    "#scale training and test data\n",
    "x_train_s, x_test_s = ss.transform(x_train), ss.transform(x_test)\n",
    "#reinitialize optimized RFC\n",
    "rfc_optimized = RandomForestClassifier(n_estimators = 30, criterion = 'gini', max_depth = 6, max_features = 'auto',\n",
    "                                      bootstrap = True, n_jobs = -1)\n",
    "#fit on training data\n",
    "rfc_optimized.fit(x_train_s, y_train)\n",
    "#predict on test data\n",
    "y_pred = rfc_optimized.predict(x_test_s)\n",
    "#score on validation set\n",
    "accuracy_rfc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy of optimized RandomForestClassifier with scaled features is {}'.format(accuracy_rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is still below that of the baseline LogisticRegression model. NEXT: need to try new algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

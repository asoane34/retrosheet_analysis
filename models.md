# Modeling

## KPI / Evaluation Metrics

Looking from the outside in, it would seem that generating a gambling model based on team wins and team losses is a simple binary classification model. Binary classification has many different metrics to evaluate model performance: Accuracy, precision, F1-score, ROC Area Under the Curve, etc. However, none of these are sufficient in evaluating a gambling model. It is more than possible to achieve a high accuracy (relatively, accuracy in predicting baseball outcomes is never that high) and still produce a betting scheme that loses money. The FiveThirtyEight predictions demonstrate this point perfectly. FiveThirtyEight, as mentioned previously, maintains the MLB Elo model which is an extremely popular and successful baseball model and is considered somewhat of a gold standard in predictive power. For the 2019 baseball season, the FiveThirtyEight model accurately forecasts $59.82\%$ of baseball games. While building my own models that strictly predict game outcome, please see __SGDClassifiers.ipynb__, I was able to predict with very similar results, accurately forecasting $59.99\%$ of games. But look at the profit generated by the FiveThirtyEight scheme: 

![EloBase](neural_net/bankroll_images/elo_base.png)
Even though the games were predicted with almost $60\%$ accuracy, the betting scheme still loses money on the season. The reason being is the aforementioned VIG: Betting favorites, who win more frequently, yields lower payouts. The only metric that matters in evaluating betting schemes is bettor profit, and therefore this will be the only metric considered in evaluating models. 

## Methodology

There are many different ways to frame and approach this problem, and I have tried nearly all of them. The problem can be framed as a Regression problem, a Binary Classification problem, or a Multi-Class classification problem. While I would enjoy discussing all the different methods and their respective shortcomings and strengths, in the interest of time I will only discuss methods that showed promising results 

## Deep Learning - Binary Classification with Custom Loss Function

Standard loss functions used in Binary Classification, such as the Log-Loss function and Hinge loss, are not optimized for the task at hand. One way to work around this is to write a loss function that teaches a model how to identify good bets versus bad bets instead of wins versus losses. This loss function looks like this: 

$L\bigg(y, \hat{y}\bigg) = \displaystyle\sum_{i}^{n}\bigg(P_{1}^{(i)}*y\bigg)*ReLU\bigg(P_{1}^{(i)}*\hat{y}\bigg) + \bigg(P_{2}^{(i)}*(y-1)\bigg)*ReLU\bigg(P_{2}^{(i)}*(1 - \hat{y})\bigg)$

where 

* $P_{1}^{(i)}, P_{2}^{(i)}$ are the profit of a correct bet placed on the home team and road team respectively. 
* $ReLU = \begin{cases} x & 0 \leq x \lt \infty \\ 0 &  x \lt 0 \end{cases}$

The $ReLU$ function (Rectified Linear Unit) simulates a betting strategy as it only "places" a bet when the value is greater than $0$ and whereas other loss functions punish inaccurate outcome prediction, this loss function punishes bets with no value. An example to illustrate this: Placing a 100 dollar bet on a $-340$ betting favorite has a potential payout of $29$ dollars if the betting favorite wins, while the potential is still $100$. There is limited value to winning such a bet with the same amount of risk as betting on a $+140$ underdog, which would payout $140$ dollars. Now, obviously, the $-340$ favorite will win more often than the $+140$ underdog, but this loss function searches for the bets with value.  



## Regression - Line Prediction 

There are two ways to frame this as a regression problem. The first is to attempt to predict the final score of the game. This failed miserably: please see __01regression_attempt.ipynb__. However, there was a different way to frame the problem that proved more effective. Instead of predicting the outcome of the game, I built a model to predict the betting line and then generated a betting strategy using this model. 